# -*- coding: utf-8 -*-
"""전처리 및 lda모델링.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1smyrT7roupJD7DIuuYu8WBuYFlmOR4nB

#전처리 입니다.
"""

import pandas as pd

file_path = '/content/drive/My Drive/Colab Notebooks/tesla2101.csv'
df = pd.read_csv(file_path)
# 'media'와 'url' 열 삭제
df = df.drop(['media', 'url'], axis=1)
df
# 또는
# df = df.drop(columns=['media', 'url'])

import re, unicodedata
from string import whitespace
pattern_whitespace = re.compile(f'[{whitespace}]+')

# NaN 값을 빈 문자열로 대체
df['content'] = df['content'].fillna('').astype(str)
# df['subtitle'] = df['subtitle'].fillna('').astype(str)

# 공백 처리 및 정규화
pattern_whitespace = re.compile(f'[{whitespace}]+')

df['content'] = df['content'].str.replace(
    pattern_whitespace, ' '
).map(lambda x: unicodedata.normalize('NFC', x)).str.strip()

# 바이라인 제거
'''
...했다. 지역=ㅇㅇㅇ 기자, 이메일 주소, ⓒ©, www.example.com
'''
def clean_byline(text):
    # byline
    pattern_email = re.compile(r'[-_0-9a-z]+@[-_0-9a-z]+(?:\.[0-9a-z]+)+', flags=re.IGNORECASE)
    pattern_url = re.compile(r'(?:https?:\/\/)?[-_0-9a-z]+(?:\.[-_0-9a-z]+)+', flags=re.IGNORECASE)
    pattern_others = re.compile(r'\.([^\.]*(?:기자|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|큐레이터|저작권|평론가|©|©|ⓒ|\@|\/|=|:앞쪽_화살표:|무단|전재|재배포|금지|\[|\]|\(\))[^\.]*)$')
    result = pattern_email.sub('', text)
    result = pattern_url.sub('', result)
    result = pattern_others.sub('.', result)
    # 본문 시작 전 꺽쇠로 쌓인 바이라인 제거
    pattern_bracket = re.compile(r'^((?:\[.+\])|(?:【.+】)|(?:<.+>)|(?:◆.+◆)\s)')
    result = pattern_bracket.sub('', result).strip()
    return result
df['content'] = df['content'].map(clean_byline)

df

"""#LDA모델링 입니다."""

pip install gensim

pip install pyLDAvis

"""#gensim 모듈 방법입니다."""

# from gensim import corpora, models
# import gensim

# # 토큰화
# text_data = df['content'].apply(lambda x: x.split())

# df
# # 불용어 제거
# stop_words = ['your', 'stop', 'words', 'here']  # 불용어 목록을 업데이트하세요
# text_data = text_data.apply(lambda x: [word for word in x if word not in stop_words])

# # BoW 변환
# dictionary = corpora.Dictionary(text_data)
# corpus = [dictionary.doc2bow(text) for text in text_data]

# # LDA 모델 생성
# lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)

# # 토픽 시각화
# import pyLDAvis.gensim
# pyLDAvis.enable_notebook()
# pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)

from collections import Counter
words_list = df['content'].str.split()
all_word = [word for words in words_list for word in words]
word_count = Counter(all_word)
most_commos_words = word_count.most_common(10)
most_commos_words

pip install konlpy

"""#Okt모듈 입니다."""

# from konlpy.tag import Okt
# import pandas as pd

# # 형태소 분석기 초기화
# okt = Okt()

# # 데이터프레임에서 content 열을 가져와서 토큰화
# df['tokenized_content'] = df['content'].apply(lambda x: okt.morphs(x))

# # 결과 확인
# print(df['tokenized_content'])

"""#gpt lda모델링 형태소분석 추가 입니다."""

# from gensim import corpora
# from gensim.models import LdaModel, TfidfModel
# from konlpy.tag import Okt  # KoNLPy에서 형태소 분석기를 가져옴

# # 형태소 분석기 초기화
# okt = Okt()

# # 텍스트 데이터를 리스트로 변환
# documents = df['content'].tolist()

# # 각 문서를 형태소 분석 및 토큰화
# tokenized_documents = []

# for document in documents:
#     # 형태소 분석 수행 후 명사만 선택 (원하는 형태소 선택 가능)
#     tokens = [word for word, pos in okt.pos(document) if pos in ['Noun']]
#     tokenized_documents.append(tokens)

# # 사전 (Dictionary) 생성
# dictionary = corpora.Dictionary(tokenized_documents)

# # Tfidf 모델 생성
# tfidf = TfidfModel(dictionary=dictionary)
# corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_documents]

# # LDA 모델 생성
# lda_model = LdaModel(corpus, num_topics=30, id2word=dictionary, passes=15)

# # LDA 모델 출력
# for idx, topic in lda_model.print_topics(-1):
#     print(f"Topic #{idx}: {topic}")

# # # 문서 별 토픽 예측
# # for i, tokens in enumerate(tokenized_documents):
# #     bow = dictionary.doc2bow(tokens)
# #     topic = lda_model[bow]
# #     print(f"Document #{i}: {topic}")

"""#형태소분석후 불용어 처리한 모델 입니다."""

from konlpy.tag import Okt
from gensim import corpora
from gensim.models import LdaModel, TfidfModel


# 형태소 분석기 초기화
okt = Okt()

# 텍스트 데이터를 리스트로 변환
documents = df['content'].tolist()

# 불용어 리스트 정의
stop_words = ["것", "수", "이", "그", "를", "를", "등", "과", "에", "가", '때', '의', '및']

# 각 문서를 형태소 분석 및 토큰화하고 불용어 제거
tokenized_documents = []

for document in documents:
    # 형태소 분석 수행 후 명사만 선택 (원하는 형태소 선택 가능)
    tokens = [word for word, pos in okt.pos(document) if pos in ['Noun'] and word not in stop_words]
    tokenized_documents.append(tokens)

# 사전 (Dictionary) 생성
dictionary = corpora.Dictionary(tokenized_documents)

# Tfidf 모델 생성
tfidf = TfidfModel(dictionary=dictionary)
corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_documents]

# LDA 모델 생성
lda_model = LdaModel(corpus, num_topics=30, id2word=dictionary, passes=15)

# LDA 모델 출력
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic #{idx}: {topic}")

"""#LDA 토픽모델링 방법입니다.(형태소X)"""

# from gensim import corpora
# from gensim.models import LdaModel, TfidfModel

# # 텍스트 데이터를 리스트로 변환
# documents = df['content'].tolist()



# # 각 문서를 토큰화 (단어 리스트로 분할)
# tokenized_documents = [document.split() for document in documents]

# # 사전 (Dictionary) 생성
# dictionary = corpora.Dictionary(tokenized_documents)

# # Tfidf 모델 생성
# tfidf = TfidfModel(dictionary=dictionary)
# corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]

# # LDA 모델 생성
# lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)

# # LDA 모델 출력
# for idx, topic in lda_model.print_topics(-1):
#     print(f"Topic #{idx}: {topic}")

# # 문서 별 토픽 예측
# for i, doc in enumerate(tokenized_documents):
#     bow = dictionary.doc2bow(doc)
#     topic = lda_model[bow]
#     print(f"Document #{i}: {topic}")

"""#여기부터 김현진 강사님 코드 입니다."""

from gensim import corpora
from gensim.models import LdaModel, TfidfModel

tokenized_documents = []
for doc in documents:
  tokenized_documents.append(doc.split(' '))
tokenized_documents

id2word = corpora.Dictionary(tokenized_documents)
print(id2word)
id2word

for value in id2word:
  print(value, id2word[value])

corpus_TDM = []
for doc in tokenized_documents:
  result = id2word.doc2bow(doc)
  corpus_TDM.append(result)

corpus_TDM

tfidf = TfidfModel(corpus_TDM)
corpus_TFIDF = tfidf[corpus_TDM]

n = 100
lda = LdaModel(corpus=corpus_TFIDF,
               id2word=id2word,
               num_topics=n,
               random_state=100)

for t in lda.print_topics() :
  print(t[0],':',t[1])