{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj9UZ_PWwTlE"
      },
      "source": [
        "# 1. Collecting (데이터 수집 및 클렌징)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxcz6qm7BPb-"
      },
      "outputs": [],
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# 파일 경로 지정\n",
        "os.chdir('/content/drive/MyDrive/Project_Tesla')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlFbMzFujd2l",
        "outputId": "648b8721-853f-42cd-9476-6c6aec9c6237"
      },
      "outputs": [],
      "source": [
        "# 모듈 임포트 선언\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re, unicodedata\n",
        "from string import whitespace\n",
        "\n",
        "\n",
        "# 네이버뉴스 1개 크롤링\n",
        "def news(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title_element = soup.select_one('h2#title_area')\n",
        "    title = title_element.text if title_element else None\n",
        "    date_element = soup.select_one('span.media_end_head_info_datestamp_time')\n",
        "    date = date_element.get('data-date-time') if date_element else None\n",
        "    content_element = soup.select_one('article#dic_area')\n",
        "    content = content_element.text.strip() if content_element else None\n",
        "    return {\n",
        "        'title': title,\n",
        "        'date': date,\n",
        "        'content': content\n",
        "    }\n",
        "\n",
        "\n",
        "# 네이버뉴스 페이지+원하는 날짜 크롤링\n",
        "def news_list(keyword, startdate, enddate):\n",
        "    li = []\n",
        "    h = {'User-Agent': '...',\n",
        "         'Referer': '...',\n",
        "         'cookie': '...'}\n",
        "    for d in pd.date_range(startdate, enddate):\n",
        "        str_d = d.strftime(\"%Y.%m.%d\")\n",
        "        page = 1\n",
        "        print(str_d)\n",
        "        while True:\n",
        "            start = (page - 1) * 10 + 1\n",
        "            print(page)\n",
        "            URL = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={0}&sort=2&photo=0&field=0&pd=3&ds={1}&de={2}&mynews=0&office_type=0&office_section_code=0&news_office_checked=&office_category=0&service_area=0&nso=so:r,p:from{3}to{4},a:all&start={5}\".format(keyword, str_d, str_d, str_d.replace(\".\", \"\"), str_d.replace(\".\", \"\"), start)\n",
        "            res = requests.get(URL, headers=h)\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            if soup.select_one(\".api_noresult_wrap\"):\n",
        "                break\n",
        "            news_list = soup.select(\"ul.list_news li\")\n",
        "            for item in news_list:\n",
        "                if len(item.select(\"div.info_group a\")) == 2:\n",
        "                    li.append(news(item.select(\"div.info_group a\")[1]['href']))\n",
        "            page = page + 1\n",
        "    return pd.DataFrame(li, columns=['title', 'date', 'content'])\n",
        "\n",
        "\n",
        "# 바이라인 제거\n",
        "def clean_byline(text):\n",
        "    # 바이라인\n",
        "    pattern_email = re.compile(r'[-_0-9a-z]+@[-_0-9a-z]+(?:\\.[0-9a-z]+)+', flags=re.IGNORECASE)\n",
        "    pattern_url = re.compile(r'(?:https?:\\/\\/)?[-_0-9a-z]+(?:\\.[-_0-9a-z]+)+', flags=re.IGNORECASE)\n",
        "    pattern_others = re.compile(r'\\.([^\\.]*(?:기자|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|큐레이터|저작권|평론가|©|©|ⓒ|\\@|\\/|=|▶|무단|전재|재배포|금지|\\[|\\]|\\(\\))[^\\.]*)$')\n",
        "    result = pattern_email.sub('', text)\n",
        "    result = pattern_url.sub('', result)\n",
        "    result = pattern_others.sub('.', result)\n",
        "\n",
        "    # 본문 시작 전 꺽쇠로 쌓인 바이라인 제거\n",
        "    pattern_bracket = re.compile(r'^((?:\\[.+\\])|(?:【.+】)|(?:<.+>)|(?:◆.+◆)\\s)')\n",
        "    result = pattern_bracket.sub('', result).strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# 크롤링할 데이터 (키워드, 시작날짜, 종료날짜)\n",
        "result_df = news_list('테슬라', '2022.08.01', '2022.08.31')\n",
        "\n",
        "\n",
        "# 크롤링 데이터, 데이터 프레임에 저장 및 필요없는 column 삭제\n",
        "df = pd.DataFrame(result_df)\n",
        "df['content'] = df['content'].fillna('').astype(str).map(clean_byline)\n",
        "\n",
        "\n",
        "# 유니코드 문자 전처리 및 정규 표현식 사용\n",
        "pattern_whitespace = re.compile(f'[{whitespace}]+')\n",
        "df['content'] = df['content'].str.replace(pattern_whitespace, ' ').map(lambda x: unicodedata.normalize('NFC', x)).str.strip()\n",
        "\n",
        "\n",
        "# 클렌징 데이터 csv 파일로 저장\n",
        "df.to_csv('[2022-Aug]news_data_cleansing.csv', index=False, encoding='utf-8-sig')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFvD4QRiweQD"
      },
      "source": [
        "# 2. Preprocessing (형태소 분석, 불용어 처리)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0jzwqm08XBu"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xrTi4v4XwgfP"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "from gensim import corpora\n",
        "\n",
        "\n",
        "# csv 파일 불러오기\n",
        "df = pd.read_csv('[2022-Aug]news_data_cleansing.csv')\n",
        "\n",
        "\n",
        "# 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "\n",
        "# 텍스트 데이터를 리스트로 변환\n",
        "documents = df['content'].tolist()\n",
        "\n",
        "\n",
        "# 불용어 리스트 정의\n",
        "stop_words = [\"것\", \"수\", \"이\", \"그\", \"를\", \"를\", \"등\", \"과\", \"에\", \"가\", '때', '의', '및']\n",
        "\n",
        "\n",
        "# 각 문서를 형태소 분석 및 토큰화하고 불용어 제거\n",
        "tokenized_documents = []\n",
        "for document in documents:\n",
        "    # 형태소 분석 수행 후 명사만 선택 (원하는 형태소 선택 가능)\n",
        "    tokens = [word for word, pos in okt.pos(str(document)) if pos in ['Noun'] and word not in stop_words]\n",
        "    tokenized_documents.append(tokens)\n",
        "\n",
        "\n",
        "# 사전 (Dictionary) 생성\n",
        "dictionary = corpora.Dictionary(tokenized_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbFt29B1zzIN"
      },
      "source": [
        "# 3. Analysis (토픽 모델링)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al7KT-JLz3Rw",
        "outputId": "f181f3fc-9ae6-4931-dedb-dd9cfd96af23"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel, TfidfModel\n",
        "\n",
        "\n",
        "# Tfidf 모델 생성\n",
        "tfidf = TfidfModel(dictionary=dictionary)\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_documents]\n",
        "\n",
        "\n",
        "# LDA 모델 생성\n",
        "lda_model = LdaModel(corpus, num_topics=30, id2word=dictionary, passes=15)\n",
        "\n",
        "\n",
        "# LDA 모델 출력\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic #{idx}: {topic}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlJtXBIs-DhK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
